---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a tenure-track Assistant Professor in the Information Systems Technology and Design (ISTD) Pillar at [Singapore University of Technology and Design](https://www.sutd.edu.sg) (SUTD). Prior to joining SUTD, I was a Research Fellow in Computer Vision and Robot Perception Lab, Department of Computer Science, [National University of Singapore](http://www.nus.edu.sg/) (NUS). I recieved my Ph.D. in Computer Science from NUS in March 2021, supervised by [Professor Tat-Seng Chua](https://www.chuatatseng.com/). 

I am heading [Intelligent Machine Perception Lab](https://impl2023.github.io) (IMPL) at SUTD, with a primary focus on, but not limited to: 
<!---(1) **Computer Vision**: 3D computer vision, (3D) scene understanding, 3D reconstruction, 3D generation and editing, spatial intelligence. -->
(1) **3D Computer Vision**: 3D scene understanding, 3D reconstruction, 3D generation and editing.
(2) **Machine Learning**: data-efficient learning, multi-modal learning, continual learning, out-of-distribution learning, robust learning.
(3) **Embodied AI**: multi-modal perception, spatial intelligence, embodied navigation, embodied manipulation.

_______________________________________________________________________________________________________
<h3>
  <a name="positions"></a> Open Positions
</h3>
<div class="highlight"> 
  <ul>
  <li>I am looking for <strong>PhD applicants</strong> with strong background in computer science, fully-supported by <a href="https://sutd.edu.sg/Admissions/Graduate/Scholarships">SUTD</a>/<a href="https://aisingapore.org/research/phd-fellowship-programme/">AISG</a>/industry scholarship.</li>
  <li>I am recruiting <strong>research fellow</strong> (holds a doctoral degree) with relevant research experience on computer vision.</li>
  <li>I am welcoming self-fund or <a href="https://www.csc.edu.cn/chuguo/">CSC-fund</a> <strong>visiting PhD/Master students</strong> and <strong>local MComp/BComp students</strong> with interests in the area of (3D) computer vision and machine learning.</li>  
  </ul>
  <i>Please check <a href="https://impl2023.github.io/joinus">here</a> for more information, and feel free to reach out via email if you are interested in working with me.</i>
</div>  

_______________________________________________________________________________________________________
<h3>
  <a name="news"></a> News
</h3>
<div class="mini">
  <ul>
  <li> <strong>[Jan 2026]</strong> I am invited to serve as senior Area Chair at IEEE ICME 2026!</li> 
  <li> <strong>[Jan 2026]</strong> One paper about incremental few-shot semantic segmentation is accepted by TIP 2026!</li> 
  <li> <strong>[Dec 2025]</strong> I am invited to serve as Publicity Chair at ACM ICMR 2026!</li> 
  <li> <strong>[Nov 2025]</strong> Two papers about point cloud representation learning and radar-LiDAR scene flow estimation are accepted by AAAI 2026, for oral and poster presentations, respectively!</li>
  <li> <strong>[Oct 2025]</strong> I am invited to give a keynote talk at the workshop on <i>Multimodal Foundation Models for Spatial Intelligence</i> at ACM Multimedia 2025!</li>
  <li> <strong>[Oct 2025]</strong> I am invited serve as an Associate Editor for <i>IEEE Transactions on Circuits and Systems for Video Technology</i> (Impact Factor: 11.1)!</li> 
  <li> <strong>[Sep 2025]</strong> One paper about 3D fine-grained embodied reasoning is accepted by NeurIPS 2025!</li>
  <li> <strong>[Sep 2025]</strong> I am invited to give a talk at NEXUS Japanâ€“Singapore Joint Workshop 2025!</li>
  <li> <strong>[Aug 2025]</strong> I will serve as an Area Chair for ICLR 2026!</li> 
  <li> <strong>[Jul 2025]</strong> One paper about assumptive reasoning in MLLMs is accepted by MM 2025!</li>
  <li> <strong>[Jun 2025]</strong> Four papers are accepted by ICCV 2025! </li> 
  <li> <strong>[May 2025]</strong> One paper about multi-modal 3D panoptic segmentation is accepted by ICML 2025!</li> 
  <li> <strong>[Apr 2025]</strong> One paper about multi-view clutering is accepted by IJCAI 2025!</li> 
  <li> <strong>[Apr 2025]</strong> I will serve as an Area Chair for MM 2025!</li> 
  <li> <strong>[Mar 2025]</strong> One paper about occluded human reconstruction is accepted by ICME 2025! </li>  
  <li> <strong>[Feb 2025]</strong> Two papers about active 3D object detection and embodied multi-agent collaboration are accepted by CVPR 2025!</li> 
  <li> <strong>[Feb 2025]</strong> One paper about 3D object detection for autonomous driving is accepted by IJCV 2025! </li> 
  <li> <strong>[Feb 2025]</strong> I am invitated to serve as an Associate Editor for <i>Knowledge-Based Systems</i> (Impact Factor: 7.6)!</li> 
  <li> <strong>[Feb 2025]</strong> One paper about semi-supervised medical domain generalization is accepted by TMM 2025!</li> 
  <li> <strong>[Jan 2025]</strong> One paper about 3D reconstruction and editing is accepted by ICLR 2025!</li> 
  <li> <strong>[Dec 2024]</strong> I will serve as an Area Chair for NLPCC 2025!</li> 
  <li> <strong>[Dec 2024]</strong> One paper about 3D visual grounding is accepted by AAAI 2025!</li> 
  <li> <strong>[Dec 2024]</strong> One paper about class-incremental 3D object detection is accepted by TIP 2024!</li> 
  <li> <strong>[Nov 2024]</strong> I will serve as a senior PC for IJCAI 2025!</li> 
  <li> <strong>[Nov 2024]</strong> I am awarded a grant titled "<i>Bridging Language and Physical Real-world for 3D Reasoning and Object Manipulation</i>" from TL@SUTD as the sole Principal Investigator!</li>
  <li> <strong>[Oct 2024]</strong> I am invited to serve as Demo Chair at ACM Multimedia 2025!</li> 
  <li> <strong>[Oct 2024]</strong> One paper about open-set single-source domain generalization is accepted by TMM 2024!</li>
  <li> <strong>[Sep 2024]</strong> I am awarded a joint SMU-SUTD grant titled "<i>Synthesis and Resilience: Generative Models for Generalizable 3D World Understanding</i>" as the co-Principal Investigator!</li>
  <li> <strong>[Sep 2024]</strong> I will serve as an Area Chair for ICLR 2025!</li> 
  <li> <strong>[Aug 2024]</strong> I am awarded a MoE Tier 2 grant titled "<i>Empowering Real-World 3D Scene Understanding: Navigating Noise, Distribution Shifts, and Incremental Learning</i>" as the sole Principal Investigator!</li>
  <li> <strong>[Aug 2024]</strong> I am appointed as a Technical Committee member for IEEE-CAS Multimedia Systems and Applications!</li>
  <li> <strong>[Jul 2024]</strong> Two papers about domain generalized 3D semantic segmentation and UDA for 3D object detection are accepted by BMVC 2024!</li>
  <li> <strong>[Jul 2024]</strong> Two papers about generalizable neural semantic fields and point cloud representation learning are accepted by MM 2024!</li>
  <li> <strong>[Jul 2024]</strong> Two papers about open-vocabulary 3D object detection and 3D Gaussain splatting editing are accepted by ECCV 2024!</li>
  <li> <strong>[Jan 2024]</strong> One paper about language-guided 3D affordance segmentation is accepted by CVPR 2024!</li>
  <li> <strong>[Jan 2024]</strong> One paper about semi-supervised 3D instance segmentation is accepted by ICRA 2024!</li>
  <li> <strong>[Dec 2023]</strong> I am awarded a grant titled "<i>MANTIS - Cross-modality Resiliency against Real-world Attacks</i>" from DSO as the sole Principal Investigator!</li>
  <li> <strong>[Dec 2023]</strong> Two papers about semi-supervised 3D object detection and robust visual recognition are accepted by AAAI 2024!</li>
  <li> <strong>[Oct 2023]</strong> One paper about self-supervised point cloud representation learning is accepted by 3DV 2024 as an oral paper!</li>
  <li> <strong>[Sep 2023]</strong> One paper about visual domain generalization is accepted by IJCV 2023!</li>
  <li> <strong>[Aug 2023]</strong> One paper about robust few-shot point cloud segmentation is accepted by BMVC 2023!</li>
  <li> <strong>[Aug 2023]</strong> I am awarded a grant titled "<i>Towards Realistic Deep Learning for 3D Vision</i>" from A*STAR as the co-Investigator!</li>
  <li> <strong>[Jul 2023]</strong> One paper about generalized few-shot point cloud segmentation is accepted by ICCV 2023!</li>
  <li> <strong>[Jun 2023]</strong> One paper about 6-DoF grasps synthesis is accepted by IROS 2023!</li>
  <li> <strong>[May 2023]</strong> One paper about monocular 3D object detection is accepted by TCSVT 2023!</li>
  <li> <strong>[Mar 2023]</strong> I am invited to serve as Demo Chair at Sixth IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR 2023)!</li>
  <li> <strong>[Feb 2023]</strong> I am invited to join the Organising Committee of IEEE ICME 2023 Workshop on 3D Multimedia Analytics, Search and Generation!</li>
  <li> <strong>[Oct 2022]</strong> I am awarded a grant titled "<i>Multi-modal Joint Learning for Scene Understanding</i>" from SUTD-ZJU IDEA as the sole Principal Investigator!</li>
  <li> <strong>[Sep 2022]</strong> I am awarded a grant titled "<i>Data-efficient 3D Object Detection for Robot Perception</i>" from TL@SUTD as the sole Principal Investigator!</li>
  <li> <strong>[Aug 2022]</strong> I join the Singapore University of Technology and Design as an Assistant Professor!</li>
  <li> <strong>[Jul 2022]</strong> Three papers are accepted by ECCV 2022!</li>
  <li> <strong>[Dec 2021]</strong> One paper about class-incremental 3D object detection is accepted by AAAI 2022 as an oral paper!</li>
  <li> <strong>[Jun 2021]</strong> I am selected for the CVPR 2021 Doctoral Consortium. My mentor is Prof. Serge Belongie!</li>
  <li> <strong>[May 2021]</strong> I win the <strong>IMDA Excellent Prize</strong> (best thesis) for my PhD thesis!</li>
  <li> <strong>[Mar 2021]</strong> I successfully defended my PhD thesis "Towards Learning Scene Semantics on 3D Point Clouds"!</li>
  <li> <strong>[Mar 2021]</strong> One paper about few-shot 3D semantic segmentation is accepted by CVPR 2021!</li>
  <li> <strong>[Aug 2020]</strong> I recieve the <strong>Research Achievement Award</strong> from SoC!</li>
  <li> <strong>[Feb 2020]</strong> One paper about semi-supervised 3D object detection is accepted by CVPR 2020 as an oral paper!</li>
  </ul>
</div>


<style>
table, th, td {
  border: none;
  border-collapse: collapse;
}
</style>

_______________________________________________________________________________________________________
<h3>
  <a name="Publications"></a> Selected Publications
</h3>
<span style="font-size:15px">Please visit [my google scholar profile](https://scholar.google.com/citations?user=KOL2dMwAAAAJ&hl=en&oi=ao) for the full publication list.</span>
<span><br></span><i style="color:#0000FF; font-size:13.5px">* indicates corresponding author, and # indicates co-corresponding author</i>

 <font face="helvetica, ariel, &#39;sans serif&#39;">
        <table cellspacing="0" cellpadding="0" class="noBorder">
          <tbody>
          <tr>
                   <td class="noBorder" width="40%">
                        <img width="320" src="../images/ifss-diff.png" border="0">
                    </td>
                    <td>
                      <b>Towards Generative Understanding: Incremental Few-shot Semantic Segmentation with Diffusion Models</b>
                      <br>
                      Qun Li, Lu Huang, Fu Xiao, <strong>Na Zhao</strong>, Bir Bhanu 
                      <br>
                      <em>IEEE Transactions on Image Processing (TIP), 2026 </em> 
                      <br>
                      [<a href="https://ifss-diff.github.io">Project</a>] [<a href="https://ieeexplore.ieee.org/document/11353366">Paper</a>] [<a href="https://github.com/totoropink/iFSS-Diff">Code</a>]
                    </td>
          </tr>
          <tr>
                   <td class="noBorder" width="40%">
                        <img width="320" src="../images/RaLiFlow_framework.png" border="0">
                    </td>
                    <td>
                      <b>RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds</b>
                      <br>
                      Jingyun Fu, Zhiyu Xiang#, <strong>Na Zhao#</strong>
                      <br>
                      <em>40th AAAI Conference on Artificial Intelligence, 2026</em> 
                      <br>
                      [<a href="https://github.com/FuJingyun/RaLiFlow">Preprint</a>] [<a href="https://github.com/FuJingyun/RaLiFlow">Code</a>]
                    </td>
          </tr>
          <tr>
                   <td class="noBorder" width="40%">
                        <img width="320" src="../images/GPS_framework.png" border="0">
                    </td>
                    <td>
                      <b>Graph Smoothing for Enhanced Local Geometry Learning in Point Cloud Analysis</b>
                      <br>
                      Shangbo Yuan, Jie Xu, Ping Hu, Xiaofeng Zhu, <strong>Na Zhao</strong> 
                      <br>
                      <em>40th AAAI Conference on Artificial Intelligence, 2026</em> <i style="color:#e74d3c">Oral Presentation</i>
                      <br>
                      [<a>Paper (Coming soon)</a>] [<a href="https://github.com/shangboyuan/GSPoint">Code</a>]
                    </td>
          </tr>
          <tr>
                   <td class="noBorder" width="40%">
                        <img width="320" src="../images/Affordbot_teaser.png" border="0">
                    </td>
                    <td>
                      <b>AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</b>
                      <br>
                      Xinyi Wang, Xun Yang#, Yanlong Xu, Yuchen Wu, Zhen Li, <strong>Na Zhao#</strong>
                      <br>
                      <em>39th Annual Conference on Neural Information Processing Systems (NeurIPS), 2025</em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2511.10017">Preprint</a>] [<a href="https://github.com/hannahwxy/AffordBot">Code</a>]
                    </td>
          </tr>
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/Motionlab_teaser.png" border="0">
                    </td>
                    <td>
                      <b>MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm</b>
                      <br>
                      Ziyan Guo, Zeyu HU, De Wen Soh, <strong>Na Zhao*</strong>
                      <br>
                      <em>International Conference on Computer Vision (ICCV), 2025</em> 
                      <br>
                      [<a href="https://diouo.github.io/motionlab.github.io/">Project</a>][<a href="https://arxiv.org/pdf/2502.02358">Preprint</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_MotionLab_Unified_Human_Motion_Generation_and_Editing_via_the_Motion-Condition-Motion_ICCV_2025_paper.pdf">Paper</a>] [<a href="https://github.com/Diouo/MotionLab">Code</a>]
                    </td>
          </tr>
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/H3R_framework.png" border="0">
                    </td>
                    <td>
                      <b>H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction</b>
                      <br>
                      Heng Jia, Linchao Zhu, <strong>Na Zhao</strong>
                      <br>
                      <em>International Conference on Computer Vision (ICCV), 2025</em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2508.03118">Preprint</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Jia_H3R_Hybrid_Multi-view_Correspondence_for_Generalizable_3D_Reconstruction_ICCV_2025_paper.pdf">Paper</a>] [<a href="https://github.com/JiaHeng-DLUT/H3R">Code</a>]
                    </td>
          </tr>
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/RML_framework.png" border="0">
                    </td>
                    <td>
                      <b>Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation</b>
                      <br>
                      Jie Xu, <strong>Na Zhao#</strong>, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu#
                      <br>
                      <em>International Conference on Computer Vision (ICCV), 2025</em> 
                      <br>
                      [<a href="https://www.arxiv.org/pdf/2503.04151">Preprint</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Robust_Multi-View_Learning_via_Representation_Fusion_of_Sample-Level_Attention_and_ICCV_2025_paper.pdf">Paper</a>] [<a href="https://github.com/SubmissionsIn/RML">Code</a>]
                    </td>
          </tr>
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/VGPCC_framework.png" border="0">
                    </td>
                    <td>
                      <b>Geometric Alignment and Prior Modulation for View-Guided Point Cloud Completion on Unseen Categories</b>
                      <br>
                      Jingqiao Xiu, Yicong Li, <strong>Na Zhao</strong>, Han Fang, Xiang Wang, Angela Yao
                      <br>
                      <em>International Conference on Computer Vision (ICCV), 2025</em> 
                      <br>
                      [<a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Xiu_Geometric_Alignment_and_Prior_Modulation_for_View-Guided_Point_Cloud_Completion_ICCV_2025_paper.pdf">Paper</a>]
                    </td>
          </tr>
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/MARS_framework.png" border="0">
                    </td>
                    <td>
                      <b>Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning</b>
                      <br>
                      Yian Li, Wentao Tian, Yang Jiao, Tianwen Qian, <strong>Na Zhao</strong>,  Bin Zhu, Jingjing Chen, Yu-Gang Jiang
                      <br>
                      <em>ACM Multimedia (MM), 2025</em> 
                      <br>
                      [<a href="https://dl.acm.org/doi/10.1145/3746027.3754720">Paper</a>]
                    </td>
          </tr>
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/IAL_framework.png" border="0">
                    </td>
                    <td>
                      <b>How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation</b>
                      <br>
                      Yining Pan, Qiongjie Cui, Xulei Yang, <strong>Na Zhao*</strong>
                      <br>
                      <em>International Conference on Machine Learning (ICML), 2025</em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2505.18956">Preprint</a>] [<a href="https://openreview.net/pdf/ea38ded40d57a840cbde86fb7bfa9588256ea489.pdf">Paper</a>] [<a href="https://github.com/IMPL-Lab/IAL">Code</a>]
                    </td>
           </tr>
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/OcSplats_framework.png" border="0">
                    </td>
                    <td>
                      <b>OcSplats: Rendering Occluded Humans with Prior Knowledge</b>
                      <br>
                      Jie Zhang, Qiongjie Cui, Xulei Yang, <strong>Na Zhao*</strong>
                      <br>
                      <em>IEEE International Conference on Multimedia & Expo (ICME), 2025</em> 
                      <br>
                      [<a href="https://ieeexplore.ieee.org/document/11210056">Paper</a>]
                    </td>
           </tr>  
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/UMD_framework.png" border="0">
                    </td>
                    <td>
                      <b>Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection</b>
                      <br>
                      Jiangyi Wang, <strong>Na Zhao*</strong>
                      <br>
                      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025</em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2503.16125">Preprint</a>] [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Uncertainty_Meets_Diversity_A_Comprehensive_Active_Learning_Framework_for_Indoor_CVPR_2025_paper.pdf">Paper</a>] [<a href="https://github.com/JoeWang-0519/CVPR25_UMD">Code</a>]
                    </td>
           </tr> 
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/CoTS_framework.png" border="0">
                    </td>
                    <td>
                      <b>Collaborative Tree Search for Enhancing Embodied Multi-Agent Collaboration</b>
                      <br>
                      Lizheng Zu, Lin Lin, Song Fu, <strong>Na Zhao</strong>, Pan Zhou
                      <br>
                      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025</em> 
                      <br>
                      [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zu_Collaborative_Tree_Search_for_Enhancing_Embodied_Multi-Agent_Collaboration_CVPR_2025_paper.pdf">Paper</a>]
                    </td>
           </tr>  
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/CT3D_framework.png" border="0">
                    </td>
                    <td>
                      <b>CT3D++: Improving 3D Object Detection with Keypoint-induced Channel-wise Transformer</b>
                      <br>
                      Hualian Sheng, Sijia Cai, <strong>Na Zhao</strong>, Bing Deng, Qiao Liang, Min-Jian Zhao, Jieping Ye 
                      <br>
                      <em>International Journal on Computer Vision (IJCV), 2025</em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2406.08152">Preprint</a>] [<a href="https://link.springer.com/article/10.1007/s11263-025-02404-8">Paper</a>] [<a href="https://github.com/hlsheng1/CT3D-plusplus">Code</a>]
                    </td>
          </tr> 
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/DAC_teaser.png" border="0">
                    </td>
                    <td>
                      <b>Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization</b>
                      <br>
                      Jincai Song, Haipeng Chen, Jun Qin#, <strong>Na Zhao#</strong>
                      <br>
                      <em>IEEE Transactions on Multimedia (TMM), 2025</em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2509.20785">Preprint</a>]  [<a href="https://ieeexplore.ieee.org/document/11175547">Paper</a>] 
                    </td>
           </tr> 
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/GaussianBlock_framework.png" border="0">
                    </td>
                    <td>
                      <b>GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians</b>
                      <br>
                      Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, <strong>Na Zhao*</strong>
                      <br>
                      <em>International Conference on Learning Representations (ICLR), 2025 </em> 
                      <br>
                      [<a href="https://arxiv.org/abs/2410.01535">Preprint</a>] [<a href="https://github.com/Jiangshuyi0V0/GaussianBlock">Code</a>]
                    </td>
           </tr> 
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/AugRefer_framework.png" border="0">
                    </td>
                    <td>
                      <b>AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and Spatial Relation-based Referring</b>
                      <br>
                      Xinyi Wang, <strong>Na Zhao*</strong>, Zhiyuan Han, Dan Guo, Xun Yang
                      <br>
                      <em>Thirty-Ninth AAAI Conference on Artificial Intelligence, 2025 </em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2501.09428">Preprint</a>] [<a href="https://dl.acm.org/doi/10.1609/aaai.v39i8.32863">Paper</a>] [<a>Code (Coming soon) </a>]
                    </td>
           </tr> 
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/DEBUG_teaser.png" border="0">
                    </td>
                    <td>
                      <b>Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization</b>
                      <br>
                      Pengkun Jiao, <strong>Na Zhao#</strong>, Jingjing Chen#, Yu-Gang Jiang
                      <br>
                      <em>IEEE Transactions on Multimedia (TMM), 2025 </em> 
                      <br>
                      [<a href="https://arxiv.org/abs/2411.02920">Preprint</a>] [<a href="https://ieeexplore.ieee.org/abstract/document/10855521">Paper</a>] 
                    </td>
           </tr> 
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/GS2-GNeSF_framework.png" border="0">
                    </td>
                    <td>
                      <b>GS^2-GNeSF: Geometry-Semantics Synergy for Generalizable Neural Semantic Fields</b>
                      <br>
                      Chengshun Wang, <strong>Na Zhao*</strong>
                      <br>
                      <em>ACM Multimedia (MM), 2024 </em> 
                      <br>
                      [<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681156">Paper</a>] 
                    </td>
          </tr> 
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/OPFR_framework.png" border="0">
                    </td>
                    <td>
                      <b>On-the-fly Point Feature Representation for Point Clouds Analysis</b>
                      <br>
                      Jiangyi Wang, Zhongyao Cheng, <strong>Na Zhao#</strong>, Jun Cheng, Xulei Yang#
                      <br>
                      <em>ACM Multimedia (MM), 2024 </em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2407.21335">Preprint</a>] [<a href="https://dl.acm.org/doi/pdf/10.1145/3664647.36807005">Paper</a>] 
                    </td>
          </tr> 
          <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/INHA_teaser.png" border="0">
                    </td>
                    <td>
                      <b>Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image</b>
                      <br>
                      Pengkun Jiao, <strong>Na Zhao*</strong>, Jingjing Chen, Yu-Gang Jiang
                      <br>
                      <em>European Conference on Computer Vision (ECCV), 2024 </em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2407.05256">Preprint</a>] [<a href="https://link.springer.com/chapter/10.1007/978-3-031-73195-2_22">Paper</a>] 
                    </td>
           </tr>
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/VCEdit_teaser.png" border="0">
                    </td>
                    <td>
                      <b>View-Consistent 3D Editing with Gaussian Splatting</b>
                      <br>
                      Yuxuan Wang, Xuanyu Yi, Zike Wu, <strong>Na Zhao</strong>, Long Chen, Hanwang Zhang
                      <br>
                      <em>European Conference on Computer Vision (ECCV), 2024 </em> 
                      <br>
                      [<a href="https://arxiv.org/abs/2403.11868">Preprint</a>] [<a href="https://link.springer.com/chapter/10.1007/978-3-031-72761-0_23">Paper</a>]  [<a href="https://github.com/Yuxuan-W/vcedit">Code</a>]
                    </td>
              </tr>
            <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/laso_teaser.png" border="0">
                    </td>
                    <td>
                      <b>LASO: Language-guided Affordance Segmentation on 3D Object</b>
                      <br>
                      Yicong Li, <strong>Na Zhao#</strong>, Junbin Xiao, Chun Feng, Xiang Wang#, Tat-Seng Chua
                      <br>
                      <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024 </em> 
                      <br>
                      [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf">Paper</a>] [<a href="https://github.com/yl3800/LASO">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/pcteacher_framework.png" border="0">
                    </td>
                    <td>
                      <b>End-to-End Semi-Supervised 3D Instance Segmentation with PCTeacher</b>
                      <br>
                      Linfeng Li, <strong>Na Zhao*</strong>
                      <br>
                      <em>IEEE International Conference on Robotics and Automation (ICRA), 2024 </em> 
                      <br>
                      [<a href="https://ieeexplore.ieee.org/abstract/document/10610145">Paper</a>] 
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/DPKE_results.png" border="0">
                    </td>
                    <td>
                      <b>Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection</b>
                      <br>
                      Yucheng Han, <strong>Na Zhao*</strong>, Weiling Chen, Keng-Teck Ma, Hanwang Zhang   
                      <br>
                      <em>Thirty-Eighth AAAI Conference on Artificial Intelligence, 2024 </em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2401.05011.pdf">Preprint</a>] [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/27976">Paper</a>] [<a href="https://github.com/tingxueronghua/DPKE">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/rltr_teaser.png" border="0">
                    </td>
                    <td>
                      <b>Robust Visual Recognition with Class-Imbalanced Open-World Noisy Data</b>
                      <br>
                      <strong>Na Zhao*</strong>, Gim Hee Lee
                      <br>
                      <em>Thirty-Eighth AAAI Conference on Artificial Intelligence, 2024 </em> 
                      <br>
                      [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29642">Paper</a>] [<a href="https://github.com/Na-Z/LIOND">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/SDCoT++_framework.png" border="0">
                    </td>
                    <td>
                      <b>SDCoT++: Improved Static-Dynamic Co-Teaching for Class-Incremental 3D Object Detection</b>
                      <br>
                      <strong>Na Zhao</strong>, Peisheng Qian, Fang Wu, Xun Xu, Xulei Yang, Gim Hee Lee
                      <br>
                      <em>IEEE Transactions on Image Processing (TIP), 2024 </em> 
                      <br>
                      [<a href="https://ieeexplore.ieee.org/document/10819355">Paper</a>] [<a>Code (Coming soon) </a>]
                    </td>
               </tr> 
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/OHDA_framework.png" border="0">
                    </td>
                    <td>
                      <b>Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection</b>
                      <br>
                      Yunsong Wang, <strong>Na Zhao</strong>, Gim Hee Lee 
                      <br>
                      <em>The British Machine Vision Conference (BMVC), 2024 </em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2406.11311">Preprint</a>] [<a href="https://github.com/wangys16/OHDA">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/CINMix_framework.png" border="0">
                    </td>
                    <td>
                      <b>Synthetic-to-Real Domain Generalized Semantic Segmentation for 3D Indoor Point Clouds</b>
                      <br>
                      Yuyang Zhao, <strong>Na Zhao</strong>, Gim Hee Lee 
                      <br>
                      <em>The British Machine Vision Conference (BMVC), 2024 </em> 
                      <br>
                      [<a href="https://arxiv.org/pdf/2212.04668">Preprint</a>] 
              </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/GRL_framework.png" border="0">
                    </td>
                    <td>
                      <b>Enhancing Generalizability of Representation Learning for Data-Efficient 3D Scene Understanding</b>
                      <br>
                      Yunsong Wang, <strong>Na Zhao</strong>, Gim Hee Lee 
                      <br>
                      <em>International Conference on 3D Vision (3DV), 2024 </em> <i style="color:#e74d3c"> Oral Presentation </i>
                      <br>
                      [<a href="https://arxiv.org/pdf/2406.11283">Preprint</a>] [<a href="https://ieeexplore.ieee.org/abstract/document/10550664">Paper</a>] 
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/Shade2_framework.png" border="4px">
                    </td>
                    <td>
                      <b>Style-Hallucinated Dual Consistency Learning: A Unified Framework for Visual Domain Generalization</b>
                      <br>
                      Yuyang Zhao, Zhun Zhong, <strong>Na Zhao</strong>, Nicu Sebe, Gim Hee Lee
                      <br>
                      <em>International Journal on Computer Vision (IJCV), 2023 </em>
                      <br>
                      [<a href="https://arxiv.org/pdf/2212.09068.pdf">Preprint</a>] [<a href="https://link.springer.com/article/10.1007/s11263-023-01911-w">Paper</a>]  [<a href="https://github.com/HeliosZhao/SHADE-VisualDG">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/R3DFSSeg_framework.png" border="4px">
                    </td>
                    <td>
                      <b>Towards Robust Few-shot Point Cloud Semantic Segmentation</b>
                      <br>
                      Yating Xu, <strong>Na Zhao</strong>, Gim Hee Lee
                      <br>
                      <em>The British Machine Vision Conference (BMVC), 2023 </em>
                      <br>
                      [<a href="https://arxiv.org/pdf/2309.11228.pdf">Preprint</a>] [<a href="https://papers.bmvc2023.org/0081.pdf">Paper</a>] [<a href="https://github.com/Pixie8888/R3DFSSeg">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/GFS3DSeg_framework.png" border="4px">
                    </td>
                    <td>
                      <b>Generalized Few-Shot Point Cloud Segmentation Via Geometric Words</b>
                      <br>
                      Yating Xu, Conghui Hu, <strong>Na Zhao</strong>, Gim Hee Lee
                      <br>
                      <em>International Conference on Computer Vision (ICCV), 2023 </em>
                      <br>
                      [<a href="https://arxiv.org/pdf/2309.11222.pdf">Preprint</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Generalized_Few-Shot_Point_Cloud_Segmentation_via_Geometric_Words_ICCV_2023_paper.pdf">Paper</a>]  [<a href="https://github.com/Pixie8888/GFS-3DSeg_GWs">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/Graspflow_framework.png" border="0">
                    </td>
                    <td>
                      <b>Refining 6-DoF Grasps with Context-Specific Classifiers</b>
                      <br>
                      Tasbolat Taunyazov, Heng Zhang, John Patrick Eala, <strong>Na Zhao</strong>, Harold Soh
                      <br>
                      <em>International Conference on Intelligent Robots and Systems (IROS), 2023 </em>
                      <br>
                      [<a href="https://arxiv.org/pdf/2308.06928.pdf">Preprint</a>] [<a href="https://github.com/tasbolat1/graspflow">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/PDR_framework.png" border="0">
                    </td>
                    <td>
                      <b>PDR: Progressive Depth Regularization for Monocular 3D Object Detection </b>
                      <br>
                      Hualian Sheng, Sijia Cai, <strong>Na Zhao</strong><sup>#</sup>, Bing Deng, Min-Jian Zhao<sup>#</sup>, Gim Hee Lee
                      <br>
                      <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023 </em>
                      <br>
                      [<a href="https://ieeexplore.ieee.org/abstract/document/10124735">Paper</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/SOFT_framework.png" border="0">
                    </td>
                    <td>
                      <b>Teaching with Soft Label Smoothing for Mitigating Noisy Labels in Facial Expressions </b>
                      <br>
                      Tohar Lukov, <strong>Na Zhao</strong>, Gim Hee Lee, Ser-Nam Lim
                      <br>
                      <em>European Conference on Computer Vision (ECCV), 2022 </em>
                      <br>
                      [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720639.pdf">Paper</a>]
                      [<a href="https://github.com/toharl/soft">Code</a>]
                      <br>
                    </td>
              </tr>  
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/RDIoU_framework.png" border="0">
                    </td>
                    <td>
                      <b>Rethinking IoU-based Optimization for Single-stage 3D Object Detection </b>
                      <br>
                      Hualian Sheng, Sijia Cai, <strong>Na Zhao*</strong>, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Min-Jian Zhao, Gim Hee Lee 
                      <br>
                      <em>European Conference on Computer Vision (ECCV), 2022 </em>
                      <br>
                      [<a href="https://arxiv.org/pdf/2207.09332">Preprint</a>] [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690536.pdf">Paper</a>]
                      [<a href="https://github.com/hlsheng1/RDIoU">Code</a>]
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/SHADE_teaser.png" border="0">
                    </td>
                    <td>
                      <br>
                      <b>Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation </b>
                      <br>
                      Yuyang Zhao, Zhun Zhong, <strong>Na Zhao</strong>, Nicu Sebe, Gim Hee Lee
                      <br>
                      <em>European Conference on Computer Vision (ECCV), 2022 </em>
                      <br>
                      [<a href="https://arxiv.org/pdf/2204.02548">Preprint</a>] [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880530.pdf">Paper</a>]
                      [<a href="https://github.com/HeliosZhao/SHADE">Code</a>]
                      <br>
                    </td>
              </tr>
              <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/SDCoT_teaser.png" border="0">
                    </td>
                    <td>
                      <br>
                      <b>Static-Dynamic Co-Teaching for Class-Incremental 3D Object Detection </b>
                      <br>
                      <strong>Na Zhao</strong>, Gim Hee Lee
                      <br>
                      <em>Thirty-Sixth AAAI Conference on Artificial Intelligence, 2022 </em><i style="color:#e74d3c">Oral Presentation</i>
                      <br>
                      [<a href="https://arxiv.org/abs/2112.07241">Preprint</a>] [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/20254">Paper</a>]
                      [<a href="https://github.com/Na-Z/SDCoT">Code</a>]
                      <br>
                    </td>
              </tr>
	            <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/FS3DSS_framework.png" border="0">
                    </td>
                    <td>
                      <br>
	                    <b>Few-shot 3D Point Cloud Semantic Segmentation </b>
	                    <br>
	                    <strong>Na Zhao</strong>, Tat-Seng Chua, Gim Hee Lee
	                    <br>
	                    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021 </em>
	                    <br>
            			    [<a href="https://arxiv.org/pdf/2006.12052.pdf">Preprint</a>] [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Few-Shot_3D_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.pdf">Paper</a>]
            			    [<a href="https://github.com/Na-Z/attMPTI">Code</a>]
            			    <!---[<a href="https://www.youtube.com/watch?v=i5X1L1_03Rs">Video</a>]-->
                      <br>
                    </td>
              </tr>
	            <tr>
                    <td width="40%">
                        <img width="320" src="../images/SESS_teaser.png" border="0">
                    </td>
                    <td>
                      <br>
	                    <b>SESS: Self-Ensembling Semi-Supervised 3D Object Detection </b>
	                    <br>
	                    <strong>Na Zhao</strong>, Tat-Seng Chua, Gim Hee Lee
	                    <br>
	                    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020  </em><i style="color:#e74d3c">Oral Presentation</i>
	                    <br>
			                [<a href="https://arxiv.org/pdf/1912.11803.pdf">Preprint</a>] [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_SESS_Self-Ensembling_Semi-Supervised_3D_Object_Detection_CVPR_2020_paper.pdf">Paper</a>]
            			    [<a href="https://github.com/Na-Z/sess">Code</a>]
            			    <!---[<a href="https://www.youtube.com/watch?v=AGJsp4aksS0">Video</a>]-->
                      <br>
                    </td>
              </tr>
			        <tr>
                    <td width="40%">
                        <img width="320" src="../images/PS2Net_teaser.png" border="0">
                    </td>
                    <td>
                        <br>
                        <b>PS^2-Net: A Locally and Globally Aware Network for Point-Based Semantic Segmentation</b>
                        <br>
                        <strong>Na Zhao</strong>, Tat-Seng Chua, Gim Hee Lee
                        <br>
                        <em>25th International Conference on Pattern Recognition (ICPR), 2020 </em>
                        <br>
                  			[<a href="https://arxiv.org/pdf/1908.05425.pdf">Preprint</a>] [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412875">Paper</a>] 
                  			[<a href="https://github.com/Na-Z/PS-2Net">Code</a>]
                  			<!---[<a href="https://www.youtube.com/watch?v=IupewGCU0o8">Video</a>]-->
                        <br>
                    </td>
               </tr>  
            	</tbody>
            </table>
</font>


_______________________________________________________________________________________________________

<h3>
  <a name="intership"></a> Research Grants
</h3>
<div class="mini">
      <ul>
      <li>Principal Investigator. TL@SUTD Seed Grant. <span style="color: #3d85c6">S$200,000</span>. Mar 2025 - Mar 2027.
       <span><br></span>Topic: <i>Bridging Language and Physical Real-world for 3D Reasoning and Object Manipulation</i>
       </li> 
       <li>Principal Investigator. MoE Tier 2 Research Grant. <span style="color: #3d85c6">S$994,411</span>. Feb 2025 - Feb 2028.
       <span><br></span>Topic: <i>Empowering Real-World 3D Scene Understanding: Navigating Noise, Distribution Shifts, and Incremental Learning</i>
       </li>   
       <li>co-Principal Investigator. AISG Research Grant. <span style="color: #3d85c6">S$999,999</span>. Jan 2025 - Jan 2028.
       <span><br></span>Topic: <i>Sequential Deepfake Model Attribution</i>
       </li>  
       <li>SUTD Principal Investigator. SMU-SUTD Joint Research Grant. <span style="color: #3d85c6">S$275,000</span>. Nov 2024 - Oct 2026.
       <span><br></span>Topic: <i>Synthesis and Resilience: Generative Models for Generalizable 3D World Understanding</i>
       <!---<span><br></span>Principal Investigator: <a href="https://panzhous.github.io">Prof. Zhou Pan</a>, Singapore Management University (SMU)-->
       </li>   
       <li>Principal Investigator. DSO Research Grant. <span style="color: #3d85c6">S$998,000</span>. Dec 2023 - Dec 2026.
       <span><br></span>Topic: <i>Cross-modality Resiliency against Real-world Attacks</i>
       </li>
       <li>Co-Investigator. A*STAR MTC Programmatic Grant. <span style="color: #3d85c6">S$9,773,400</span>. Aug 2023 - Jul 2026.
        <!---S$599,950-->
       <span><br></span>Topic: <i>Towards Realistic Deep Learning for 3D Vision</i>
       </li>
       <li>Principal Investigator. SUTD-ZJU Thematic Research Grant. <span style="color: #3d85c6">S$148,187</span>. Dec 2022 - Nov 2024.
       <span><br></span>Topic: <i>Multi-modal Joint Learning for Scene Understanding</i>
       <!---<span><br></span>Collaborator: <a href="https://yiyiliao.github.io">Prof. Liao Yiyi</a>, Zhejiang University (ZJU)-->
       </li>
       <li>Principal Investigator. TL@SUTD Seed Grant. <span style="color: #3d85c6">S$85,000</span>. Oct 2022 - Apr 2024.
       <span><br></span>Topic: <i>Data-efficient 3D Object Detection for Robot Perception</i>
      </li>
      </ul>
</div>

_______________________________________________________________________________________________________

<h3>
  <a name="intership"></a> Academic Experience
</h3>
<div class="mini">
      <ul>
       <li>Research Fellow. <a>Computer Vision and Robotic Perception Laboratory</a>, NUS. Apr 2021 - Jul 2022.</li>
       <li>Research Associate. <a>Computer Vision and Robotic Perception Laboratory</a>, NUS. Jan 2021 - Mar 2021.</li>
       <li>Research Assistant. <a href="https://nextcenter.org/">NExT++ Rearch Center</a>, NUS. Aug 2015 - Dec 2016.</li>
      </ul>
</div>


_______________________________________________________________________________________________________
<h3>
  <a name="services"></a> Academic Services
</h3>
<div class="mini">
  <ul>
  <li> <strong>Conference Reviewer</strong>: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, SIGGRAPH, AAAI, IJCAI, MM, etc</li>
  <!---NeurIPS 2023-2024, ECCV 2024, ICML 2024, CVPR 2021-2024, ICLR 2024, IJCAI 2021-2024, ICCV 2023, AAAI 2021-2023, BMVC 2022, ECCV 2022, ICCV 2021, MM 2019-2020, PCM 2018-->
  <li> <strong>Journal Reviewer</strong>: TPAMI, TIP, TKDE, RAL, TCSVT, TOMM, TMM, RAL, Multimedia Systems, etc</li>
  <!---Transactions on Knowledge and Data Engineering, Transactions on Image Processing, Transactions on Circuits and Systems for Video Technology, Pattern Recognition, Transactions on Multimedia Computing, Communications, and Applications, Journal of Photogrammetry and Remote Sensing, Transactions on Multimedia, Multimedia Systems, Neurocomputing, Journal of Visual Communication and Image Representation-->
  <li> <strong>Organizer</strong>: The 16th ACM International Conference on Multimedia Retrieval (Publicity Chair), The 33rd ACM International Conference on Multimedia 2025 (Demo Chair), The 6th IEEE International Conference on Multimedia Information Processing and Retrieval 2023 (Demo Chair), The 2nd ICME Workshop on 3D Multimedia Analytics, Search and Generation 2023 (Chair), The 22nd international conference on Multimedia Modeling 2016 (Publication Chair)</li>
  <li> <strong>Technical Committee Member</strong>: IEEE-CAS Multimedia Systems & Applications (2024-2028)</li>
  <li> <strong>Jounral Associate Editor (AE)</strong>: Knowledge-Based Systems (Feb 2025-), IEEE TCSVT (Nov 2025-)</li>
  <li> <strong>Conference Senior Area Chair (SAC)</strong>: ICME 2026 </li>
  <li> <strong>Conference Area Chair (AC)</strong>: ICLR 2026, MM 2025, ICLR 2025, NLPCC 2025 </li>
  <li> <strong>Conference Senior Program Committee (PC) Member</strong>: IJCAI 2025</li>
  </ul>
</div>


_______________________________________________________________________________________________________
<h3>
  <a name="teaching"></a> Teaching Experience
</h3>
<div class="mini">
  <ul>
  <li> Mentor, 01.400 Capstone (3), Fall 2025 & Spring 2026 </li>
  <li> Mentor, 01.400 Capstone (2), Fall 2024 & Spring 2025 </li>
  <li> Instructor, 50.007 Machine Learning, Spring 2023/2024/2025/2026. </li>
  <li> Mentor, 01.400 Capstone (11), Fall 2023 & Spring 2024. </li>
  <li> Instructor, 10.020 Data Driven World, Fall 2023. </li>
  <li> Teaching Assistant, CS4242 Social Media Computing, Spring 2018 & Spring 2019.</li>
  <li> Teaching Assistant, CS5340 Uncertainty Modeling in AI, Fall 2018.  </li>
  <li> Teaching Assistant, CG3002 Embedded Systems Design Project, Fall 2017. </li>
  </ul>
</div>
